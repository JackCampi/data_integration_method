% =======================
%  Two-column paper style
% =======================
\documentclass[11pt,a4paper]{article}

% ---------- Language & encoding ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% ---------- Geometry ----------
\usepackage[a4paper,margin=1.5cm]{geometry}

% ---------- Fonts ----------
\usepackage{newtxtext,newtxmath} % Times-like academic font
\usepackage{microtype}

% ---------- Math ----------


% ---------- Figures / tables ----------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

% ---------- Links / bibliography ----------
\usepackage[hidelinks]{hyperref}
\usepackage[numbers,sort&compress]{natbib}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection}{0.6em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.6em}{}
\titlespacing*{\section}{0pt}{1.0ex}{0.8ex}
\titlespacing*{\subsection}{0pt}{0.8ex}{0.6ex}

% ---------- Header ----------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\small Data integration for kidney transplantation}
\rhead{\small \thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ---------- Abstract box ----------
\usepackage{framed}
\setlength{\FrameSep}{8pt}

% ---------- No indentation ----------
\setlength{\parindent}{15pt}
\setlength{\parskip}{4pt}
\setlength{\columnsep}{0.8cm}

% ==================================
%  Title
% ==================================
\title{\Large \textbf{Data Integration Methods and Application to Kidney Transplantation}\\
\vspace{0.3em}\normalsize Final Project -- Mathématiques Appliquées}

\author{
Angel ROMERO, Ana María PINZÓN
\thanks{École Centrale de Nantes, MATHAPPLI} \\
\small {École Centrale de Nantes}
\small {MATHAPPLI}
}

\date{\today}

\begin{document}


% ---------- Two-column title block ----------
\twocolumn[
\maketitle
\vspace{-1.0em}

\noindent\rule{\linewidth}{1.2pt}

\vspace{0.2em}
%\begin{center}
\small
\textbf{Keywords:}  multi-modal data integration, multi-view learning, transplantation, prediction
%\end{center}

%\vspace{0.2em}
% \begin{framed}
% \noindent\textbf{Abstract.}
% Dans ce rapport, nous étudions les méthodes d’intégration de données hétérogènes
% (cliniques, génétiques, biologiques) dans un cadre d’apprentissage supervisé.
% L’objectif est de construire un prédicteur du rejet humoral après transplantation
% rénale. Après une synthèse de la littérature, nous appliquons une méthode
% d’intégration spécifique et évaluons ses performances sur des données réelles.
% \end{framed}

\vspace{1.2em}
]

% ==================================
%  Content
% ==================================

\section{Introduction}


% Pb amount of data
Predictive statistical models have historically faced significant difficulties in being implemented in the medical field due to the heterogeneity of biological data. This poses a challenge for unification within a single model, since the data structures are very different in nature.

% Pb multi-modal data
In this project, we will work with data coming from different sources, such as clinical, demographic, and molecular data. The primary problem is how to relate these variables when they do not exist in the same vector spaces. This is a problem of multi-modal data integration, which has been extensively researched in recent years.

% Multi-omics data
Latest technologies in biology laboratories allow for molecular data collection at levels we never had before. Nowadays, we talk about multi-omics data, which analyses biological information by levels of deepness. This information comes from data sets such as the genome, proteome, transcriptome, epigenome, etc. Which is why it receives his name. This adds up to our multi-modal integration problem.

% Possible discussion for improving pred. due to amount of info on an individu
Another difficulty lies in the fact that there are few individuals in the databases. This could be due to the lack of standardization in many lab procedures, the cost of data collection, and the ethical issues that arise within the medical field. This poses a barrier for models that require numerous observations to be statistically valid, as they assume a large number of individuals to make accurate inferences.

Nevertheless, we do have access to different types of information for the same individual. The underlying idea is whether we could achieve better predictions by correctly relating these variables. How to leverage this to improve our models is the core question of multi-view learning.

% Problem description
For this project, we will discuss different ways of integrating multi-modal data (taking into account their nature) into a prediction model and test a few of them to evaluate if we can construct a predictor of humoral rejection after kidney transplantation.

We start by describing different types of data integration, ranging from early approaches to more modern techniques such as deep learning and kernel methods. Finally, we select the models we consider most suitable for our research problem.

\section{Data integration methods}

\subsection{First approaches}

When it comes to integrating data from different sources, the first approach we can consider is to sequential concatenation methods. The most basic method involves collecting all information into a single, complete matrix \cite{chen2023multiomics}. However, combining raw data does not always lead to a better approximation as those methods do not model the complex relationships between data across sources.

After this naive approach, some approaches have been proposed as the Bayesian data integration method proposed by Brooke L et al. \cite{fridley2012bayesian}, where they build a Bayes framework. This method uses path analysis in order to represent direct and indirect genomic effects on a phenotype. 

% EXPLICAR MATH

Other approaches use a linear combination among data sources, then employ this new combination in the classical classification/regression models. For example, Prélot L. et al. \cite{prelotYEARglycaemic} proposed a model based on the calculation of "scores" of Metabolic and Methylation data. 

Finally, although some mechanisms have been postulated to address the raw data concatenation problem, it still remains the "curse of dimensionality" as combining different scaled data can inflate hight-dimension. That is why more complex models will be discussed in the next section.  

\subsection{Modern methods and research}


\subsubsection{Deep Learninng}

Deep learning models have gained importance in recent years, as they obtain satisfactory scores on complex data relationships modeling. Throughout the years, some classical models have been adapted to the multi-modal learning problem, making the good enough to capture information from the same problem seen from different points of view. 

Even though these models can be a good alternative to solve the problematic, their nature difficults the interpretation of the phenomena. Due to the importance of interpretability in the healthcare sector, these models are rarely used, especially for making health-related decisions. 

Here we discuss how to adapt some popular models when using multiple source data, as just combining them does not lead to optimal improvement of the model \citep{yan2021deepmvl}.

\paragraph{Autoencoders}

 This model was born with the main idea of encoding an input pattern $X \in R^N$ into some reduced pattern $f(X) \in R^d$, with $ d\ll N$ \cite{rumelhart1986pdp}, where we will call $R^d$ the latent space and $f : R^N \rightarrow R^d$ the encoding function. Then, we define $g : R^d \rightarrow R^N$ as the decoding function. The autoencoder problem can be expressed as \cite{baldi2012autoencoders}:

$$
\min_{f,g} \sum_{i}^N \| g \circ f (x_i) \| 
$$

Taking the multi-modal data into account, some authors have proposed variations to the basic model. Here we present the bimodal Autoencoder proposed by Ngiam et al. \cite{Ngiam2011689} where they build the sequential model shown below:

\includegraphics[scale=0.30]{res/AE.png}

% Explicar maths

There are still some other variations of the model. For example, the Correlational AE proposed by BAE, Feng et al. \cite{feng2014crossmodal}, which minimizes the correlational learning error from each modal-data. Next, we find the $AE^2$ presented by Zhang et al. \cite{zhang2019ae2nets} which takes two AE levels. The inner layer holds an AE for each data source and the outer layer manages the multi-modal integration. 

Further proposals present diverse variations. Among these, we find the cross-modal auto-encoder (CMAE), the margin sensitive (MSAE) and the convolutional AE \cite{yan2021deepmvl}.

\paragraph{Convolutional Neural Networks} 

(CNN) were first presented by Yann LeCun et al. \cite{lecun2015deep} as a high level feature representation. 

% Explicar maths

When taking multiple sources, stabilization can be achieved through two kinds of architectures: multi-modal-one-net model which integrates all data sequentially into a CNN and the one-modal-one-net model which creates a separate CNN for each source and then fusions them with a deep layer \cite{yan2021deepmvl}, as shown in the figure below:

\includegraphics[scale=0.35]{res/CNN.png}

When using the one-modal-one-net mechanism we do not search to fusion the response of each model, but to include a combining mapping at the last convolutional layer. To this purpose, we define $f: x^{(i)}_n, x^{(j)}_n \rightarrow y_n$ as the fusion function. Christoph Feichtenhofer et al. \cite{feichtenhofer2016twostream}. proposed and tested multiple ways of CNN fusion. Here we present some commonly used mappings: 

\begin{table}[h]
\centering
\caption{Multi-modal CNN fusion functions}
\begin{tabular}{lcc}
\toprule
$f$ & math expression \\
\midrule
$y^{sum}_n$ & $y^d_n = x^{(i, d)}_n + x^{(j,d)}_n$ \\
$y^{max}_n$ & $y^d_n = \max(x^{(i, d)}_n, x^{(j,d)}_n)$ \\
$y^{cat}_n$ & $y^{2d}_n = x^{(i, d)}_n, y^{2d-1}_n = x^{(j,d)}_n$ \\
$y^{conv}_n$ & $y^{conv} = y^{cat} \ast F + b$ \\
\bottomrule
\end{tabular}
\end{table}

Where $d$ is the convolution chanel, as the $cat$ fusion function represents the concatenation of two different channels. For the convolution, $F$ stands for the filter and $b$ stands for the biases. 



\subsubsection{Kernel methods}

% Kernel methods
A kernel $K$ is a mapping that takes two elements from an arbitrary set and assigns them a real number ($K : \mathcal{X} \times \mathcal{X} \longrightarrow \mathbb{R}$). It can be viewed as a "similarity" function. The kernel function can also be written as the mapping $\phi : \mathcal{X} \longrightarrow \mathcal{F} $ which satisfies 

$$
k(x,x')=<\phi(x),\phi(x')>_\mathcal{F}
$$

The kernel matrix or Gram matrix $ \mathbf{K} \in \mathbb{R}^{n \times n} $ is the symmetric matrix $K_{ij}  = k(x_i,x_j)$ for $i=1,2,...,n$, which must be positive semi-definite \cite{mva_kernels_2022}. 

Kernel methods are algorithms that use kernel function as inputs. The idea behind is to map the data into higher-dimensional spaces where it is nicer to do machine learning. In other words, the points are mapped into a space where they are more easily separable. Interestingly, in this new space, the points are represented by their kernel function. 

% Heterogeneous data
This approach has several advantages for our problem because it doesn't make any assumptions regarding the type of data. This allows us to map elements from any general space. For instance, our points could be real numbers, chains of characters, images, and so on. This is promising for our heterogeneous type of data. However, the problem of integrating the data persists. 

Fortunately, kernel methods offer an approach to this problem. Multiple Kernel Learning (MKL) was developed to attack this sort of problems. First, a kernel matrix is computed for each feature. Then, they are combined into a final kernel matrix by minimizing an objective function.\cite{Baiao2025}. One advantage of MKL methods is that the final kernel matrix can be used for downstream analysis, supporting the analysis of influencing elements in the prediction.

In general, using multiple kernels instead of a single one boosts the model's prediction capacity. On the other hand, combining kernels nonlinearly apparently yields better results whereas when combining complex Gaussian kernels, linear methods are more reasonable\cite{gonen2011multiple}. 

% Computational cost 
Though these methods present several advantages for machine learning practitioners, kernel methods have efficiency problems due to the computational time required to handle the Gram Matrix. Whith MKL methods this involves handling several martrices, which becomes very complicated when integrating hundreds or thousands of features. 

In the literature diverse mechanism of multiple kernel combination have been explored. They could be classified depending on the approach they use to stablish the problem. First approaches defined a combining fixed rule, such as summing or multiplying kernels, while future results introduce optimization approaches, heuristic formulations and a bayesian model framework. 

\paragraph{Bayesian MKL}
The Bayesian MKL is based on the Relevance Vector Machine (RVM) which is a Bayesian interpretation of the Support Vector Machine (SVM). As with the SVM, the RVM can be "kernelized" changing the inner products by a kernel using the "kernel trick". The Bayesian MKL models have been studied in the literature due to the very high computational time they require \cite{gonen2012bayesian}. However, multiple attempts have been proposed to make it more efficient such as the \textit{Bayesian Eﬃcient Multiple Kernel Learning} \cite{gonen2012bayesian} and the \textit{Hierarchic Bayesian Models for Kernel Learning} \cite{girolami2005hierarchic}. 

According to \cite{Wilkinson2007} the Bayesian approach to biological sequence analysis has proven to be extremely useful. Though the conventional approach allows us to estimate the model parameters, it is inconvenient for certain interesting problems because it provides unsatisfactory information for analysis. The Bayesian approach assumes the parameters to be probabilistic instead of deterministic, as conventional models assume. This change in the perspective is key to solve our problem since it allows proper propagation of uncertainty accross different levels of modeling \cite{Wilkinson2007}. Thus, it establishes a coherent mathematical framework for finding the solution.

Though the incertitudes are often neglected, they are a critical aspect of biological problems, even comparable to the solution itself. Biological data tends to be noisy, scarce and expensive. Bayesian solutions offer an interesting approach to challenging inferential problems, maximising the information that can be extracted from expensive experimental data.\cite{Wilkinson2007}.

\subsubsection{Hierarchic Bayesian Model for MKL}

Thanks to Girolami et al. \cite{girolami2005hierarchic}. We got a first implementation of a bayes model in a multiple kernel learning problem. They provide the mathematical framework by implementing a variational bayes formulation for getting the parameter estimations. 

\paragraph{HerarchiHierarchic Probabilistic Model} is the first step to be defined, so we start with the bayes approach by treating parameters as random variables depending on some other random variables we call priors. i.e $\theta  \sim P(\theta \mid \lambda)$ and $\lambda \sim P(\lambda)$ 

We define the problem for $t = [t_1, \cdots, t_N ]$ inputs and $X = [x_1, \cdots, x_N]$ data points as a Kernel regression formulated by

$$
y_n = \sum^N_{m=0} \alpha_m \sum^K_{k=0} \beta_k K_k (x_m, x_n)
$$

where we can write $K_{\beta} \in \mathbf{R}^{N \times N+1}$, so the problem become 

$$
y = K_{\beta}\alpha
$$

With $\beta \in \mathbf{R}^K$ and $\alpha \in \mathbb{R}^{N+1}$

Now we use the bayes scheme presented by Girolami and showed in the following figure

\includegraphics[scale=0.25]{res/BMKL.png}

Where we place, as in the referenced document, the following prior distributions. 

\begin{align*}
    t \mid \alpha, \beta, \gamma, X &\sim N_t(K_{\beta}\alpha, I\gamma^{-1}) \\
    \gamma \mid \rho, \varrho &\sim \Gamma_{\gamma}(\rho, \varrho) \\
    \alpha \mid \phi &\sim N_{\alpha}(0, \phi^{-1}) \\
    \phi \mid \sigma, \zeta  &\sim \prod^N_{m=0} \Gamma_{\phi m}(\sigma, \zeta) \\
    \beta \mid \varphi &\sim \mathcal{D}_{\beta}(\varphi) \\
    \varphi \mid \tau, \nu &\sim \prod^K_{k=1} \Gamma_{\varphi k}(\tau, \nu) \\
\end{align*}

Where $\phi = diag(\phi_0, \cdots, \phi_N)$ and $\mathcal{D}(a)$ represents the dirichlet distribution with mean $\phi$. 

\paragraph{Posterior Over Parameters} is the second step in the Bayesian model. Even if a Gibbs sampler can be implemented, they prefer to use a variational Bayes to estimate the posterior probabilities and its momentums. 

So we first define

\begin{align*}
    \Theta &= {\alpha, \beta, \gamma, \phi, \varphi} \\
    \Psi &= {\sigma, \zeta, \rho, \varrho, \tau, \nu}
\end{align*}

Then the posterior we search for is given by,

$$
P(\Theta, \Psi, \mid t, X)
$$

Then they propose to use II Maximum Likelihood for estimating $\Phi$ and then look for $P(\Theta \mid \Psi, t, X)$

Now, thanks to the Jensen inequality we can give a lower bound to the likelihood by approximating the posterior as,

$$
P(\Theta \mid \Psi, t, X) \backsimeq q(\alpha)q(\beta)q(\gamma)q(\phi)q(\varphi)
$$

Where $q(\theta)$ represents the optimal distribution associated with the parameter $\theta$. 

Now, taking into account the distributions families of the parameters, we can maximize the lower bound due to the fact that $q(\theta) \propto exp(E_{q(\theta)}[\log P(t, \theta \mid \Psi)])$, where $E_p[a]$ represents the expectation of a random variable $a$ with respect to the density $p$. 

Introducing $\widetilde{a} = E_{q(a)} [a]$ notation and with $K_{in} \in \mathbf{R}^{N+1}$ the vector of the kernel values from the $i-th$ kernel for the data point $n$. Then the optimal distribution associated with $\alpha$ can be written as,

$$
q(\alpha) = N_{\alpha}(m_{\alpha}, \Sigma_{\alpha})
$$

Where its params are defined as,

\begin{align*}
    \Sigma_{\alpha} &= \left[ \widetilde{\gamma} \sum^K_{i=1} \sum^K_{j=1} \widetilde{\beta_i \beta_j} \sum^N_{n=1} K_in K_jn^T + \widetilde{\phi} \right]^{-1} \\
    m_{\alpha} &= \widetilde{\gamma} \Sigma_{\alpha} \sum^N_{n=1} t_n \sum^K_{k=1} \widetilde{\beta_k}K_{kn}
\end{align*}

The optimal distribution associated with $\phi$ and its moment can be expressed as,

\begin{align*}
    q(\phi) &= \prod^N_{n=1} \Gamma_{\phi m}\left( \sigma + \frac{1}{2}, \frac{1}{2}\widetilde{\alpha_m^2} + \zeta \right) \\
    \widetilde{\phi_m} &= \frac{1 + 2 \sigma}{\widetilde{\alpha^2_m} + 2 \zeta}
\end{align*}

For $\gamma$ parameter, the optimal distribution and the moment is then, 

\begin{align*}
    q(\gamma) &= \Gamma_{\gamma}\left( \frac{N}{2} + \rho, \frac{1}{2}\left\lVert \widetilde{e} \right\rVert^2  + \varrho \right) \\
    \widetilde{\gamma} &= \frac{N + 2 \rho}{\left\lVert \widetilde{e} \right\rVert^2 + 2 \varrho}
\end{align*}

Where, 

\begin{align*}
    \left\lVert \widetilde{e} \right\rVert^2 = \sum^N_{n=1} t^2_n - 2 \sum^N_{n=1} t_n \sum^K_{k=1} \widetilde{\beta_k}\widetilde{\alpha}^T K^{kn} \\ 
+ \sum^K_{i=1}\sum^K_{j=1} \widetilde{\beta_i \beta_j} \sum^N_{n=1}K_{in}^T\widetilde{\alpha \alpha^T}K_{jn}
\end{align*}

Finally, as $\beta$ and $\phi$ do not form the same distribution family, the optimal distribution cannot be found. Girolami et al propose to use importance sampling method to compute the posterior and its moment

\paragraph{Prediction} $y^*$ can be performed for a new point $x^*$ by using $P(t^* \mid x^*, t, X)$. Or, we can approximate the result by 

$$
\int P(t^* \mid x^*, t, X, \alpha, \beta, \gamma) q(\alpha)q(\beta)q(\gamma) d\alpha d\beta d\gamma 
$$

And if we take the posterior mean $\widetilde{\beta}$, $\widetilde{\gamma}$, due to the conjugate form of the posterior and the likelihood we can write the prediction as,

$$
\int P(t^* \mid x^*, t, X, \alpha, \widetilde{\beta}, \widetilde{\gamma}) q(\alpha)d\alpha 
$$

With its associated moments,

\begin{align*}
    \widetilde{t^*} &= \widetilde{\beta}^T k(x) m_{\alpha} \\
    \widetilde{\sigma^*} &= \widetilde{\gamma}^{-1} + \widetilde{\beta}^T K(x)^T \Sigma_{\alpha} K(x) \widetilde{\beta}
\end{align*}

Where $K(x) \in \mathbf{R}^{N+1 \times K}$ is the kernel values between the new data point and the whole dataset. 

\section{Conclusion}

Predictive statistical models have difficulty being implemented in the medical field due to the heterogeneity of biological data. Different types of models have been developed to tackle this problem, ranging from the naive attempt of concatenating everything into a single matrix to more modern, complex models such as deep learning and multiple kernel learning (MKL).

Some methods have proven to be efficient in the integration of multi-modal data, improving the modeling of complex relationships across sources. However, methods such as deep learning present difficulties in the interpretability of results because the latent space does not necessarily have a physical representation. This is added to the difficulty of training a neural network with only a few hundred or thousand data points.

Multiple kernel learning methods offer interesting advantages to overcome the multi-modal problem but struggle with high-dimensionality. Multiple approaches have been proposed to improve the computational efficiency of these models. Also, the resulting kernel matrix can be used for downstream analysis, supporting the identification of influencing elements in the prediction, which is crucial for making health-related decisions. This also allows us to take into consideration the risk of the decisions taken by the machine.

MKL methods show promising progress in healthcare data analysis, but there is still an important issue: the uncertainty of the result. In this regard, Bayesian inference presents a good alternative. Bayesian Multiple Kernel Learning methods seem well-adapted to the problem, offering an approachable way of integrating data, analyzing results, and obtaining the uncertainty of the predictions. This is why we will present an implementation of this model, as well as an analysis of the results and a comparison with other models.



% ==================================
%  Bibliography
% ==================================
\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
