% =======================
%  Two-column paper style
% =======================
\documentclass[11pt,a4paper]{article}

% ---------- Language & encoding ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% ---------- Geometry ----------
\usepackage[a4paper,margin=1.5cm]{geometry}

% ---------- Fonts ----------
\usepackage{newtxtext,newtxmath} % Times-like academic font
\usepackage{microtype}

% ---------- Math ----------


% ---------- Figures / tables ----------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

% ---------- Links / bibliography ----------
\usepackage[hidelinks]{hyperref}
\usepackage[numbers,sort&compress]{natbib}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection}{0.6em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.6em}{}
\titlespacing*{\section}{0pt}{1.0ex}{0.8ex}
\titlespacing*{\subsection}{0pt}{0.8ex}{0.6ex}

% ---------- Header ----------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\small Data integration for kidney transplantation}
\rhead{\small \thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ---------- Abstract box ----------
\usepackage{framed}
\setlength{\FrameSep}{8pt}

% ---------- No indentation ----------
\setlength{\parindent}{15pt}
\setlength{\parskip}{4pt}
\setlength{\columnsep}{0.8cm}

% ==================================
%  Title
% ==================================
\title{\Large \textbf{Data Integration Methods and Application to Kidney Transplantation}\\
\vspace{0.3em}\normalsize Final Project -- Mathématiques Appliquées}

\author{
Angel ROMERO, Ana María PINZÓN
\thanks{École Centrale de Nantes, MATHAPPLI} \\
\small {École Centrale de Nantes}
\small {MATHAPPLI}
}

\date{\today}

\begin{document}


% ---------- Two-column title block ----------
\twocolumn[
\maketitle
\vspace{-1.0em}

\noindent\rule{\linewidth}{1.2pt}

\vspace{0.2em}
%\begin{center}
\small
\textbf{Keywords:} data integration, multi-view learning, multi-modal, transplantation, prediction
%\end{center}

%\vspace{0.2em}
% \begin{framed}
% \noindent\textbf{Abstract.}
% Dans ce rapport, nous étudions les méthodes d’intégration de données hétérogènes
% (cliniques, génétiques, biologiques) dans un cadre d’apprentissage supervisé.
% L’objectif est de construire un prédicteur du rejet humoral après transplantation
% rénale. Après une synthèse de la littérature, nous appliquons une méthode
% d’intégration spécifique et évaluons ses performances sur des données réelles.
% \end{framed}

\vspace{1.2em}
]

% ==================================
%  Content
% ==================================

\section{Introduction}


% Pb amount of data
Predictive statistical models have had historically a lot of difficulty to be implemented  in the medical field due to the heterogeneity of the biological data. This poses a problem for the unification in a single model, since the data structures are very different in nature.
One of the main reasons is that laboratories obtain different kinds of information from the same individu such as xray images, molecular data, clinical data, etc. To sum there is not great amounts of data available because there is no standarization in many lab procedures, and also because of the cost of collecting data, and even the ethical issues that can arise from collecting data in the medical field. This poses a barrier for models which need a lot of observations to be statistically valid in the sense that they assume a huge amount of individuals to make good inferences.

% Pb multi-modal data
In this project we will work with data coming form different sources such as clinical data, demographic data, molecular data, etc. The problem here is how can we relate this variables if they do not live in the same spaces. This a problem of multi-modal data integration, which has been very researched in the later years. 

% Multi-omics data
Latest technologies in biology laboratories allow for molecular data collection at levels we never had before. Nowadays, we talk about multi-omics data, which analyses biological information by levels of deepness. This information comes from data sets such as the genome, proteome, transcriptome, epigenome, etc. Which is why it receives his name. This adds up to our multi-modal integration problem.

% Possible discussion for improving pred. due to amount of info on an individu
Another difficulty lies in the fact that there are not a lot of individuals in the data bases. Nevertheless, we do have access to different types of information for the same individual. The reasonable question would be if we could get better predictions correctly relating this variables between them. How to leverage this to improve our models is the question of the multiview learning.  

% Problem description
For this project we are gonna discuss different ways of integrating multi-modal data (taking into account their nature) in a prediction model and test a few of them to see if we can construct a predictor of humoral rejection after kidney transplantation.

We start by decribing different types of data integration, passing from the first approach to the more modern approaches such as depp learning and kernel methods. We finally, select the models we think might be more suitable for our problematic. 

\section{Data integration methods}

\subsection{First approaches}

When it comes to integrating data from different sources, the first approach we can consider is to sequential concatenation methods. The most basic method involves collecting all information into a single, complete matrix \cite{chen2023multiomics}. However, combining raw data does not always lead to a better approximation as those methods do not model the complex relationships between data across sources.

After this naive approach, some approaches have been proposed as the Bayesian data integration method proposed by Brooke L et al. \cite{fridley2012bayesian}, where they build a Bayes framework. This method uses path analysis in order to represent direct and indirect genomic effects on a phenotype. 

% EXPLICAR MATH

Other approaches use a linear combination among data sources, then employ this new combination in the classical classification/regression models. For example, Prélot L. et al. \cite{prelotYEARglycaemic} proposed a model based on the calculation of "scores" of Metabolic and Methylation data. 

Finally, although some mechanisms have been postulated to address the raw data concatenation problem, it still remains the "curse of dimensionality" as combining different scaled data can inflate hight-dimension. That is why more complex models will be discussed in the next section.  

\subsection{Modern methods and research}


\subsubsection{Deep Learninng}

Deep learning models have gained importance in recent years, as they obtain satisfactory scores on complex data relationships modeling. Throughout the years, some classical models have been adapted to the multi-modal learning problem, making the good enough to capture information from the same problem seen from different points of view. 

Even though those models can represent a good alternative to the problematic, it holds that the nature of the models keeps hard for the interpretation capacity of the phenomena, keeping them not really used, specially when taking health-related decisions. 

Here we discuss how to adapt the popular models when taking multiple source data, as just combining them cannot lead to the optimal improvement of the model \citep{yan2021deepmvl}.

\paragraph{Autoencoders}

 model was born as the main idea of encoding input pattern $X \in R^N$ into some reduced pattern $f(X) \in R^d$, with $ d\ll N$ \cite{rumelhart1986pdp}, where we will call $R^d$ the latent space and $f : R^N \rightarrow R^d$ the encoding function. Then we define $g : R^d \rightarrow R^N$ as the decoding function and the autoencoder problem can be expressed as \cite{baldi2012autoencoders}:

$$
\min_{f,g} \sum_{i}^N \| g \circ f (x_i) \| 
$$

Now, taking in mind the multi-modal data, some authors have proposed variations to the basic model. Here we present the bimodal Autoencoder proposed by Ngiam et al. \cite{Ngiam2011689} where they build the sequential model shown before:

\includegraphics[scale=0.30]{res/AE.png}

% Explicar maths

However, there still another variations of the model, such as the Correlational AE proposed by BAE, Feng et al. \cite{feng2014crossmodal}, where they try to minimize the correlational learning error from each modal-data. Right before we find the $AE^2$ presented by Zhang et al. \cite{zhang2019ae2nets} witch takes two AE levels, the inner layer holds an AE for each data source and the outer manage the multi-modal integration. 

Further proposals present diverse variations. Among these, we find the cross-modal auto-encoder (CMAE), the margin sensitive (MSAE) and the convolutional AE \cite{yan2021deepmvl}.

\paragraph{Convolutional Neural Networks} 

were first presented by Yann LeCun et al \cite{lecun2015deep} as a high level feature representation. 

% Explicar maths

When taking multiple sources, there can be stabilized two kinds of architectures: multi-modal-one-net model which integrates all data sequentially into a CNN and the one-modal-one-net model which creates separates CNN for each source and then fusion them with a deep layer \cite{yan2021deepmvl}, as shown in the figure 

\includegraphics[scale=0.35]{res/CNN.png}

When using the one-modal-one-net mechanism we do not search to fusion the response of each model, but to include a combining mapping at the last convolutional layer. To this purpose, we define $f: x^{(i)}_n, x^{(j)}_n \rightarrow y_n$ as the fusion function. Christoph Feichtenhofer et al \cite{feichtenhofer2016twostream}. proposed and tested multiple ways of CNN fusion. Here we present some used mappings. 

\begin{table}[h]
\centering
\caption{Multi-modal CNN fusion functions}
\begin{tabular}{lcc}
\toprule
$f$ & math expression \\
\midrule
$y^{sum}_n$ & $y^d_n = x^{(i, d)}_n + x^{(j,d)}_n$ \\
$y^{max}_n$ & $y^d_n = \max(x^{(i, d)}_n, x^{(j,d)}_n)$ \\
$y^{cat}_n$ & $y^{2d}_n = x^{(i, d)}_n, y^{2d-1}_n = x^{(j,d)}_n$ \\
$y^{conv}_n$ & $y^{conv} = y^{cat} \ast F + b$ \\
\bottomrule
\end{tabular}
\end{table}

Note $d$ is the convolution chanel, as cat method represent the concatenation of two different channels. For the convolution, note $F$ represents the filter and $b$ the biases. 



\subsubsection{Kernel methods}

% Kernel methods
A kernel $K$ is a mapping which takes two elements from an arbitrary space and assigns them a real number ($K : \mathcal{X} \times \mathcal{X} \longrightarrow \mathbb{R}$). It can be seen as a "similarity" function. It is symmetric since the resemblance from one element $x_i$ to $x_j$ is exactly the same as the resemblance of $x_j$ to $x_i$. The kernel function can also be seen as the mapping $\phi : \mathcal{X} \longrightarrow \mathcal{F} $ which satisfies 

$$
k(x,x')=<\phi(x),\phi(x')>_\mathcal{F}
$$

The kernel matrix or Gram matrix $ \mathbf{K} \in \mathbb{R}^{n \times n} $ is the symmetric matrix $K_{ij}  = k(x_i,x_j)$ for $i=1,2,...,n$, which must be positive semi-definite \cite{mva_kernels_2022}. 

Kernel methods are algorithms which take the kernel function as an input. The idea behind the kernel methods is to map the data into higher-dimensional spaces where it is nicer to do machine learning. Or, in other words, the points are mapped into a space where they are more easily separable. The interesting thing is that in this new space the points are now represented by their kernel function. 

% Heterogeneous data
This approach represents several advantages for our problem since it doesn't makes any assumptions regarding the type of data, allowing us to map elements from any general space. For example, our points can be real numbers, chains of characters, images, etc. This already starts to look good for our heterogenous type of data. Still, the problem of the integration of the data persists. 

Fortunately, the kernel methods have an approach for this. Multiple Kernel Learning (MKL) were conceived to attack this sort of problems. For this, a kernel matrix is computed for each feature and then they are combined into a final kernel matrix by minimizing an objective function.\cite{Baiao2025}. One advantage for MKL methods is that the final kernel matrix can be used for downstream analysis, supporting the analyses of influencing elements in the prediction.

In general, using multiple kernels instead of a single one boosts the prediction capacity of the model. On the other hand, combining kernels in a non-linear way apparently has better results whereas when combining complex Gaussian kernels, linear methods are more reasonable\cite{gonen2011multiple}. 

% Computational cost 
Though this methods present several advantages for machine learning practitioners, the kernel methods have efficincy problems du to the computational time thaa requieres the handling of the Gram Matrix. Whith the MKL methods this implies the handling of several martrices wich starts to get very complicated when integrating hundreds or thousands of features. 

In the literature diverse mechanism of multiple kernel combination have been explored. They could be classified depending on the approach they use to stablish the problem. First approaches defined a combining fixed rule, such as summing or multiplying kernels, while future results introduce optimization approaches, heuristics formulation and a bayesian model framework. 

\paragraph{Bayesian MKL}
The Bayesian MKL is based on the Relevance Vector Machine (RVM) which is a Bayesian interpretation of the Support Vector Machine (SVM). As in the SVM, the RVM can be "kernelized" changing the inner products by a kernel using the "kernel trick". The Bayesian MKL models have been studied in the literature due to the very high computational time they require \cite{gonen2012bayesian}. But multiple attempts to make it more efficient have been proposed like the \textit{Bayesian Eﬃcient Multiple Kernel Learning} \cite{gonen2012bayesian} or the \textit{Hierarchic Bayesian Models for Kernel Learning} \cite{girolami2005hierarchic}. 

According to \cite{Wilkinson2007} the Bayesian approach to biological sequence analysis has shown extremely useful. Though the conventional approach allows us to estimate the model parmeters, it is unconvinient for certain interesting problems as it is rather unsatisfactory in terms of the information provided by the analysis. The Bayesian approach suppposes the parameters to be probabilistic instead of deterministic as models have conventionnally assumed. This change in the perspective is a key benefit for our problem, since it allow proper propagation of uncertainty accross different levels of modelling \cite{Wilkinson2007}. Thus, it set a coherent mathematical framework to find the solution.

Though the incertitudes are often neglected, in biological problems it is a critical aspect of the problem, even comparable with the solution itself. Biological data tends to be noisy, scarse and expensive. Bayesian solutions offer an interesting solution to the challenging inferential problems, maximising the information that can be extracted from expensive experimental data.\cite{Wilkinson2007}.

\subsubsection{Hierarchic Bayesian Model for MKL}

Thanks to Girolami et al. \cite{girolami2005hierarchic}. We got a first implementation of a bayes model in a multiple kernel learning problem. They provide the mathematical framework by implementing a variational bayes formulation for getting the parameter estimations. 

\paragraph{HerarchiHierarchic Probabilistic Model} is the first step to be defined, so we start with the bayes approach by treating parameters as random variables depending on some other random variables we call priors. i.e $\theta  \sim P(\theta \mid \lambda)$ and $\lambda \sim P(\lambda)$ 

We define the problem for $t = [t_1, \cdots, t_N ]$ inputs and $X = [x_1, \cdots, x_N]$ data points as a Kernel regression formulated by

$$
y_n = \sum^N_{m=0} \alpha_m \sum^K_{k=0} \beta_k K_k (x_m, x_n)
$$

where we can write $K_{\beta} \in \mathbf{R}^{N \times N+1}$, so the problem become 

$$
y = K_{\beta}\alpha
$$

With $\beta \in \mathbf{R}^K$ and $\alpha \in \mathbb{R}^{N+1}$

Now we use the bayes scheme presented by Girolami and showed in the following figure

\includegraphics[scale=0.25]{res/BMKL.png}

Where we place, as in the referenced document, the following prior distributions. 

\begin{align*}
    t \mid \alpha, \beta, \gamma, X &\sim N_t(K_{\beta}\alpha, I\gamma^{-1}) \\
    \gamma \mid \rho, \varrho &\sim \Gamma_{\gamma}(\rho, \varrho) \\
    \alpha \mid \phi &\sim N_{\alpha}(0, \phi^{-1}) \\
    \phi \mid \sigma, \zeta  &\sim \prod^N_{m=0} \Gamma_{\phi m}(\sigma, \zeta) \\
    \beta \mid \varphi &\sim \mathcal{D}_{\beta}(\varphi) \\
    \varphi \mid \tau, \nu &\sim \prod^K_{k=1} \Gamma_{\varphi k}(\tau, \nu) \\
\end{align*}

Where $\phi = diag(\phi_0, \cdots, \phi_N)$ and $\mathcal{D}(a)$ represents the dirichlet distribution with mean $\phi$. 

\paragraph{Posterior Over Parameters} is the second step in the Bayesian model. Even if a Gibbs sampler can be implemented, they prefer to use a variational Bayes to estimate the posterior probabilities and its momentums. 

So we first define

\begin{align*}
    \Theta &= {\alpha, \beta, \gamma, \phi, \varphi} \\
    \Psi &= {\sigma, \zeta, \rho, \varrho, \tau, \nu}
\end{align*}

Then the posterior we search for is given by,

$$
P(\Theta, \Psi, \mid t, X)
$$

Then they propose to use II Maximum Likelihood for estimating $\Phi$ and then look for $P(\Theta \mid \Psi, t, X)$

Now, thanks to the Jensen inequality we can give a lower bound to the likelihood by approximating the posterior as,

$$
P(\Theta \mid \Psi, t, X) \backsimeq q(\alpha)q(\beta)q(\gamma)q(\phi)q(\varphi)
$$

Where $q(\theta)$ represents the optimal distribution associated with the parameter $\theta$. 

Now, taking into account the distributions families of the parameters, we can maximize the lower bound due to the fact that $q(\theta) \propto exp(E_{q(\theta)}[\log P(t, \theta \mid \Psi)])$, where $E_p[a]$ represents the expectation of a random variable $a$ with respect to the density $p$. 

Introducing $\widetilde{a} = E_{q(a)} [a]$ notation and with $K_{in} \in \mathbf{R}^{N+1}$ the vector of the kernel values from the $i-th$ kernel for the data point $n$. Then the optimal distribution associated with $\alpha$ can be written as,

$$
q(\alpha) = N_{\alpha}(m_{\alpha}, \Sigma_{\alpha})
$$

Where its params are defined as,

\begin{align*}
    \Sigma_{\alpha} &= \left[ \widetilde{\gamma} \sum^K_{i=1} \sum^K_{j=1} \widetilde{\beta_i \beta_j} \sum^N_{n=1} K_in K_jn^T + \widetilde{\phi} \right]^{-1} \\
    m_{\alpha} &= \widetilde{\gamma} \Sigma_{\alpha} \sum^N_{n=1} t_n \sum^K_{k=1} \widetilde{\beta_k}K_{kn}
\end{align*}

The optimal distribution associated with $\phi$ and its moment can be expressed as,

\begin{align*}
    q(\phi) &= \prod^N_{n=1} \Gamma_{\phi m}\left( \sigma + \frac{1}{2}, \frac{1}{2}\widetilde{\alpha_m^2} + \zeta \right) \\
    \widetilde{\phi_m} &= \frac{1 + 2 \sigma}{\widetilde{\alpha^2_m} + 2 \zeta}
\end{align*}

For $\gamma$ parameter, the optimal distribution and the moment is then, 

\begin{align*}
    q(\gamma) &= \Gamma_{\gamma}\left( \frac{N}{2} + \rho, \frac{1}{2}\left\lVert \widetilde{e} \right\rVert^2  + \varrho \right) \\
    \widetilde{\gamma} &= \frac{N + 2 \rho}{\left\lVert \widetilde{e} \right\rVert^2 + 2 \varrho}
\end{align*}

Where, 

\begin{align*}
    \left\lVert \widetilde{e} \right\rVert^2 = \sum^N_{n=1} t^2_n - 2 \sum^N_{n=1} t_n \sum^K_{k=1} \widetilde{\beta_k}\widetilde{\alpha}^T K^{kn} \\ 
+ \sum^K_{i=1}\sum^K_{j=1} \widetilde{\beta_i \beta_j} \sum^N_{n=1}K_{in}^T\widetilde{\alpha \alpha^T}K_{jn}
\end{align*}

Finally, as $\beta$ and $\phi$ do not form the same distribution family, the optimal distribution cannot be found. Girolami et al propose to use importance sampling method to compute the posterior and its moment

\paragraph{Prediction} $y^*$ can be performed for a new point $x^*$ by using $P(t^* \mid x^*, t, X)$. Or, we can approximate the result by 

$$
\int P(t^* \mid x^*, t, X, \alpha, \beta, \gamma) q(\alpha)q(\beta)q(\gamma) d\alpha d\beta d\gamma 
$$

And if we take the posterior mean $\widetilde{\beta}$, $\widetilde{\gamma}$, due to the conjugate form of the posterior and the likelihood we can write the prediction as,

$$
\int P(t^* \mid x^*, t, X, \alpha, \widetilde{\beta}, \widetilde{\gamma}) q(\alpha)d\alpha 
$$

With its associated moments,

\begin{align*}
    \widetilde{t^*} &= \widetilde{\beta}^T k(x) m_{\alpha} \\
    \widetilde{\sigma^*} &= \widetilde{\gamma}^{-1} + \widetilde{\beta}^T K(x)^T \Sigma_{\alpha} K(x) \widetilde{\beta}
\end{align*}

Where $K(x) \in \mathbf{R}^{N+1 \times K}$ is the kernel values between the new data point and the whole dataset. 

\section{Conclusion}

% Pb amount of data
Predictive statistical models have had historically a lot of difficulty to be implemented  in the medical field due to the heterogeneity of the biological data. This poses a problem for the unification in a single model, since the data structures are very different in nature.One of the main reasons is that laboratories obtain different kinds of information from the same individu such as xray images, molecular data, clinical data, etc. To sum there is not great amounts of data available because there is no standarization in many lab procedures, and also because of the cost of collecting data, and even the ethical issues that can arise from collecting data in the medical field. This presents a barrier for models which need a lot of observations to be statistically valid in the sense that they assume a huge amount of individuals to make good inferences.


Limitations (are they mathematical or simply it is not coherent with the problem?)

% How can our model trace influencing elements to give useful retour?


Rigourus enough?

Precission? 

Risk : How much risk would be tolerable for decision making in the 
medical field?

Interpretability? In the medical field, the models have to be precise and have an
accurate argument for the prediction (i.e. which indicator and why is it having
that weight on the decision taken by the model)
Finally, in practice an important question would be, how implied should be
the doctor in the decision? Should it be a model completely autonomous? 





% ======================================================================
% Ideas del gato utiles sobretodo para la segunda parte del proyecto

% \section{Introduction}
% La transplantation rénale est un traitement de référence de l’insuffisance rénale
% terminale. Cependant, le rejet humoral demeure une cause majeure d’échec du greffon.
% L’intégration de données multi-sources constitue un levier prometteur pour améliorer
% les modèles prédictifs \citep{ghojogh2021rkhs}.

% \section{Contexte et données}
% \subsection{Sources de données}
% Description des données cliniques, biologiques et génétiques utilisées, ainsi que
% les étapes de pré-traitement.

% \subsection{Problème de prédiction}
% Définition de la variable cible, horizon temporel, métriques d’évaluation.

% \section{Méthodes d’intégration de données}
% \subsection{Synthèse bibliographique}
% Early integration, late integration, intermediate integration, multi-view learning, kernel methods, CCA, PLS, etc.

% \subsection{Méthode retenue}
% Description détaillée de la méthode choisie et justification.

% \section{Résultats}
% \subsection{Protocole expérimental}
% Validation croisée, modèles de référence, réglage des hyperparamètres.

% \subsection{Résultats quantitatifs}


% \begin{table}[h]
% \centering
% \caption{Performances des modèles}
% \begin{tabular}{lcc}
% \toprule
% Modèle & AUC & Accuracy \\
% \midrule
% Clinique seul & 0.71 & 0.68 \\
% Intégration multi-vues & 0.79 & 0.74 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \section{Discussion}
% Analyse critique des résultats, limites de l’étude, interprétabilité.

% \section{Conclusion et perspectives}
% Résumé des contributions et pistes futures.



% ==================================
%  Bibliography
% ==================================
\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
