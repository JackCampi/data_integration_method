% =======================
%  Two-column paper style
% =======================
\documentclass[11pt,a4paper]{article}

% ---------- Language & encoding ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% ---------- Geometry ----------
\usepackage[a4paper,margin=1.5cm]{geometry}

% ---------- Fonts ----------
\usepackage{newtxtext,newtxmath} % Times-like academic font
\usepackage{microtype}

% ---------- Math ----------


% ---------- Figures / tables ----------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

% ---------- Links / bibliography ----------
\usepackage[hidelinks]{hyperref}
\usepackage[numbers,sort&compress]{natbib}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection}{0.6em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.6em}{}
\titlespacing*{\section}{0pt}{1.0ex}{0.8ex}
\titlespacing*{\subsection}{0pt}{0.8ex}{0.6ex}

% ---------- Header ----------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\small Data integration for kidney transplantation}
\rhead{\small \thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ---------- Abstract box ----------
\usepackage{framed}
\setlength{\FrameSep}{8pt}

% ---------- No indentation ----------
\setlength{\parindent}{15pt}
\setlength{\parskip}{4pt}
\setlength{\columnsep}{0.8cm}

% ==================================
%  Title
% ==================================
\title{\Large \textbf{Data Integration Methods and Application to Kidney Transplantation}\\
\vspace{0.3em}\normalsize Final Project -- Mathématiques Appliquées}

\author{
Angel ROMERO, Ana María PINZÓN
\thanks{École Centrale de Nantes, MATHAPPLI} \\
\small {École Centrale de Nantes}
\small {MATHAPPLI}
}

\date{\today}

\begin{document}


% ---------- Two-column title block ----------
\twocolumn[
\maketitle
\vspace{-1.0em}

\noindent\rule{\linewidth}{1.2pt}

\vspace{0.2em}
%\begin{center}
\small
\textbf{Keywords:} data integration, multi-view learning, multi-modal, transplantation, prediction
%\end{center}

%\vspace{0.2em}
% \begin{framed}
% \noindent\textbf{Abstract.}
% Dans ce rapport, nous étudions les méthodes d’intégration de données hétérogènes
% (cliniques, génétiques, biologiques) dans un cadre d’apprentissage supervisé.
% L’objectif est de construire un prédicteur du rejet humoral après transplantation
% rénale. Après une synthèse de la littérature, nous appliquons une méthode
% d’intégration spécifique et évaluons ses performances sur des données réelles.
% \end{framed}

\vspace{1.2em}
]

% ==================================
%  Content
% ==================================

\section{Introduction}


% Pb amount of data
Predictive statistical models have not been used a lot historically in the medical field due to the heterogeneity of the biological data. This means, laboratories obtain different kind of informations from the same individu such as xray images, molecular data, clinical data, etc. This poses a problem for the unification in a single model, since the data structures are very different in nature. To sum there is not great amounts of data available because there is no standarization in many lab procedures, and also because of the cost of collecting data, and even the ethical issues that can arise from collecting data in the medical field. entre otros. This presents a barrier for models which need a lot of observations to be statistically valid in the sense that they assume a huge amount of individuals to make good inferences.

% Pb multi-modal data
In this project we will work with data such as clinical data, demographic data, molecular data, etc. The problem here is how can we relate this variables if they do not live in the same spaces. This a problem of multi-modal data integration.

% Multi-omics data
Latest technologies in biology labs allow for molecular data collection at levels we never had before. Nowadays, we talk about multi-omics data, which observes multiple biological phenotypes by levels of deepness from de the genetics to the phenomics (observable disfunction).

% Possible discussion for improving pred. due to amount of info on an individu
Another difficulty lies in the fact that there are not a lot of individuals in the data base but we do have access to information of different types for the same individual. We would like to see if we can get better predictions correctly relating this variables between them. (Discusion)

% Problem description
For this project we are gonna discuss different ways of integrating multi-modal data (taking into account their nature) in a prediction model and test them to see if we can get relatively good predictions for our specific problem of the rejection humoralafter kidney transplantation.

% Other possible questions
Why do we need data integration tecniques? 
Naive approach :  Just having every variable as a simple covariate? 
Limitations (are they mathematical or simply it is not coherent with the problem?)
If its mathematical, why? (sum of distribution laws, ...)
If its coherence (Different type of data, correlation and causality,...)

% We still need to add a bit of the kidney transplant issue to contextualize the multi-modal problem in the medical field

\section{Data integration methods}

\subsection{First approach}

When talking about data integration from different sources, the first approach we can take in mind is to try sequential concatenation methods, being the basic one getting all information into a single complete matrix \cite{chen2023multiomics}. However, sometimes combining raw data cannot lead into better approximation as those methods do not model complex relationships between data across sources. 

Some approaches have been proposed, as a bayesian data integration method by Brooke L et al \cite{fridley2012bayesian}, where they build a bayes framework which uses path analysis in order to represent direct and indirect genomic effects on a phenotype. 

\textbf{EXPLICAR MATH}

Some others comprises linear combination among data sources, then they employ this new combination within the classical classification/regression models. For example, Prélot L. et al \cite{prelotYEARglycaemic} proposed the calculation of scores of Metabolic and Methylation data. 

Finally, even if some mechanisms have been postulated to fix the raw data concatenation problem, there still the "curse of dimensionality" as combining different scaled data can inflate hight-dimension. Thats why more complex models will be discussed on the next section.  

\subsection{Modern methods and research}
Kernel methods 

Others?

\subsubsection{Deep Learninng}

Deep Learning models have gained importance on the recent years, as they obtain satisfactory scores on complex data relationships modeling. through the years, some classical models have been adapted to the multi-modal learning problem, making the good enough to capture information from the same problem seen from different points of view. 

Even though those models can represent a good alternative to the problematic, it holds that the nature of the models keeps hard for the interpretation capacity of the phenomena, keeping them not really used, specially when taking health-related decisions. 

Here we discuss how to adapt the popular models when taking multiple source data, as just combining them cannot lead to the optimal improvement of the model \citep{yan2021deepmvl}.

\paragraph{Autoencoders}

 model was born as the main idea of encoding input pattern $X \in R^N$ into some reduced pattern $f(X) \in R^d$, with $ d\ll N$ \cite{rumelhart1986pdp}, where we will call $R^d$ the latent space and $f : R^N \rightarrow R^d$ the encoding function. Then we define $g : R^d \rightarrow R^N$ as the decoding function and the autoencoder problem can be expressed as \cite{baldi2012autoencoders}:

$$
\min_{f,g} \sum_{i}^N \| g \circ f (x_i) \| 
$$

Now, taking in mind the multi-modal data, some authors have proposed variations to the basic model. Here we present the bimodal Autoencoder proposed by Ngiam et al. \cite{Ngiam2011689} where they build the sequential model shown before:

\includegraphics[scale=0.30]{res/AE.png}

*Explicar maths *

However, there still another variations of the model, such as the Correlational AE proposed by BAE, Feng et al. \cite{feng2014crossmodal}, where they try to minimize the correlational learning error from each modal-data. Right before we find the $AE^2$ presented by Zhang et al. \cite{zhang2019ae2nets} witch takes two AE levels, the inner layer holds an AE for each data source and the outer manage the multi-modal integration. 

Further proposals present diverse variations. Among these, we find the cross-modal auto-encoder (CMAE), the margin sensitive (MSAE) and the convolutional AE \cite{yan2021deepmvl}.

\paragraph{Convolutional Neural Networks} 

were first presented by Yann LeCun et al \cite{lecun2015deep} as a high level feature representation. 

*Explicar maths *

When taking multiple sources, there can be stabilized two kinds of architectures: multi-modal-one-net model which integrates all data sequentially into a CNN and the one-modal-one-net model which creates separates CNN for each source and then fusion them with a deep layer \cite{yan2021deepmvl}, as shown in the figure 

\includegraphics[scale=0.35]{res/CNN.png}

When using the one-modal-one-net mechanism we do not search to fusion the response of each model, but to include a combining mapping at the last convolutional layer. To this purpose, we define $f: x^{(i)}_n, x^{(j)}_n \rightarrow y_n$ as the fusion function. Christoph Feichtenhofer et al \cite{feichtenhofer2016twostream}. proposed and tested multiple ways of CNN fusion. Here we present some used mappings. 

\begin{table}[h]
\centering
\caption{Multi-modal CNN fusion functions}
\begin{tabular}{lcc}
\toprule
$f$ & math expression \\
\midrule
$y^{sum}_n$ & $y^d_n = x^{(i, d)}_n + x^{(j,d)}_n$ \\
$y^{max}_n$ & $y^d_n = \max(x^{(i, d)}_n, x^{(j,d)}_n)$ \\
$y^{cat}_n$ & $y^{2d}_n = x^{(i, d)}_n, y^{2d-1}_n = x^{(j,d)}_n$ \\
$y^{conv}_n$ & $y^{conv} = y^{cat} \ast F + b$ \\
\bottomrule
\end{tabular}
\end{table}

Note $d$ is the convolution chanel, as cat method represent the concatenation of two different channels. For the convolution, note $F$ represents the filter and $b$ the biases. 



\subsubsection{Kernel methods}

% Kernel methods
A kernel $K$ is a mapping which takes two elements from an arbitrary space and assigns them a real number ($K : \mathcal{X} \times \mathcal{X} \longrightarrow \mathbb{R}$). It can be seen as a "similarity" function. It is symmetric since the resemblance from one element $x_i$ to $x_j$ is exactly the same as the resemblance of $x_j$ to $x_i$. The kernel function can also be seen as the mapping $\phi : \mathcal{X} \longrightarrow \mathcal{F} $ which satisfies 

$$
k(x,x')=<\phi(x),\phi(x')>_\mathcal{F}
$$

The kernel matrix or Gram matrix $ \mathbf{K} \in \mathbb{R}^{n \times n} $ is the symmetric matrix $K_{ij}  = k(x_i,x_j)$ for $i=1,2,...,n$, which must be positive semi-definite \cite{mva_kernels_2022}. 

Kernel methods are algorithms which take the kernel function as an input. The idea behind the kernel methods is to map the data into higher-dimensional spaces where it is nicer to do machine learning. Or, in other words, the points are mapped into a space where they are more easily separable. The interesting thing is that in this new space the points are now represented by their kernel function. 

% Heterogeneous data
This approach represents several advantages for our problem since it doesn't makes any assumptions regarding the type of data, allowing us to map elements from any general space. For example, our points can be real numbers, chains of characters, images, etc. This already starts to look good for our heterogenous type of data. Still, the problem of the integration of the data persists. 

Fortunately, the kernel methods have an approach for this. Multiple Kernel Learning (MKL) were conceived to attack this sort of problems. For this, a kernel matrix is computed for each feature and then they are combined into a final kernel matrix by minimizing an objective function.\cite{Baiao2025}. One advantage for MKL methods is that the final kernel matrix can be used for downstream analysis, supporting the analyses of influencing elements in the prediction.

In general, using multiple kernels instead of a single one boosts the prediction capacity of the model. On the other hand, combining kernels in a non-linear way apparently has better results whereas when combining complex Gaussian kernels, linear methods are more reasonable\cite{gonen2011multiple}. 

% Computational cost 
Though this methods present several advantages for machine learning practitioners, the kernel methods have efficincy problems du to the computational time thaa requieres the handling of the Gram Matrix. Whith the MKL methods this implies the handling of several martrices wich starts to get very complicated when integrating hundreds or thousands of features. 

In the literature diverse mechanism of multiple kernel combination have been explored. They could be classified depending on the approach they use to stablish the problem. First approaches defined a combining fixed rule, such as summing or multiplying kernels, while future results introduce optimization approaches, heuristics formulation and a bayesian model framework. 

\paragraph{Bayesian MKL}
The Bayesian MKL is based on the Relevance Vector Machine (RVM) which is a Bayesian interpretation of the Support Vector Machine (SVM). As in the SVM, the RVM can be "kernelized" changing the inner products by a kernel using the "kernel trick". The Bayesian MKL models have been studied in the literature due to the very high computational time they require \cite{gonen2012bayesian}. But multiple attempts to make it more efficient have been proposed like the \textit{Bayesian Eﬃcient Multiple Kernel Learning} \cite{gonen2012bayesian} or the \textit{Hierarchic Bayesian Models for Kernel Learning} \cite{girolami2005hierarchic}. 

According to \cite{Wilkinson2007} the Bayesian approach to biological sequence analysis has shown extremely useful. Though the conventional approach allows us to estimate the model parmeters, it is unconvinient for certain interesting problems as it is rather unsatisfactory in terms of the information provided by the analysis. The Bayesian approach suppposes the parameters to be probabilistic instead of deterministic as models have conventionnally assumed. This change in the perspective is a key benefit for our problem, since it allow proper propagation of uncertainty accross different levels of modelling \cite{Wilkinson2007}. Thus, it set a coherent mathematical framework to find the solution.

Though the incertitudes are often neglected, in biological problems it is a critical aspect of the problem, even comparable with the solution itself. Biological data tends to be noisy, scarse and expensive. Bayesian solutions offer an interesting solution to the challenging inferential problems, maximising the information that can be extracted from expensive experimental data.\cite{Wilkinson2007}.


\section{Conclusion}





% How can our model trace influencing elements to give useful retour?


Rigourus enough?

Precission? 

Risk : How much risk would be tolerable for decision making in the 
medical field?

Interpretability? In the medical field, the models have to be precise and have an
accurate argument for the prediction (i.e. which indicator and why is it having
that weight on the decision taken by the model)
Finally, in practice an important question would be, how implied should be
the doctor in the decision? Should it be a model completely autonomous? 





% ======================================================================
% Ideas del gato utiles sobretodo para la segunda parte del proyecto

% \section{Introduction}
% La transplantation rénale est un traitement de référence de l’insuffisance rénale
% terminale. Cependant, le rejet humoral demeure une cause majeure d’échec du greffon.
% L’intégration de données multi-sources constitue un levier prometteur pour améliorer
% les modèles prédictifs \citep{ghojogh2021rkhs}.

% \section{Contexte et données}
% \subsection{Sources de données}
% Description des données cliniques, biologiques et génétiques utilisées, ainsi que
% les étapes de pré-traitement.

% \subsection{Problème de prédiction}
% Définition de la variable cible, horizon temporel, métriques d’évaluation.

% \section{Méthodes d’intégration de données}
% \subsection{Synthèse bibliographique}
% Early integration, late integration, intermediate integration, multi-view learning, kernel methods, CCA, PLS, etc.

% \subsection{Méthode retenue}
% Description détaillée de la méthode choisie et justification.

% \section{Résultats}
% \subsection{Protocole expérimental}
% Validation croisée, modèles de référence, réglage des hyperparamètres.

% \subsection{Résultats quantitatifs}


% \begin{table}[h]
% \centering
% \caption{Performances des modèles}
% \begin{tabular}{lcc}
% \toprule
% Modèle & AUC & Accuracy \\
% \midrule
% Clinique seul & 0.71 & 0.68 \\
% Intégration multi-vues & 0.79 & 0.74 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \section{Discussion}
% Analyse critique des résultats, limites de l’étude, interprétabilité.

% \section{Conclusion et perspectives}
% Résumé des contributions et pistes futures.



% ==================================
%  Bibliography
% ==================================
\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
